.. _`chm-eval`:

CHM-EVAL
========

Evaluate given VCF file with chm-eval (https://github.com/lh3/CHM-eval) for benchmarking variant calling.





Example
-------

This meta-wrapper can be used in the following way:

.. code-block:: python

    rule chm_eval:
        input:
            kit="resources/chm-eval-kit",
            vcf="{sample}.vcf"
        output:
            summary="chm-eval/{sample}.summary", # summary statistics
            bed="chm-eval/{sample}.err.bed.gz" # bed file with errors
        params:
            extra="",
            build="38"
        log:
            "logs/chm-eval/{sample}.log"
        wrapper:
            "0.65.0-212-g458009d7/bio/benchmark/chm-eval"


Note that input, output and log file paths can be chosen freely.
When running with

.. code-block:: bash

    snakemake --use-conda

the software dependencies will be automatically deployed into an isolated environment before execution.




Authors
-------


* Johannes KÃ¶ster



Code
----


