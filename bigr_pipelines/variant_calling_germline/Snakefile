import logging
import os
import pandas
import sys
from pathlib import Path

worflow_source_dir = Path(next(iter(workflow.get_sources()))).absolute().parent
common = str(worflow_source_dir / "../common/python")
sys.path.append(common)

from file_manager import *
from files_linker import *
from write_yaml import *
from messages import *
from snakemake.utils import min_version
from pathlib import Path
min_version("6.0")

logging.basicConfig(
    filename="snakemake.variant_calling_germline.log",
    filemode="w",
    level=logging.DEBUG
)

container: "docker://continuumio/miniconda3:4.4.10"
localrules: bigr_copy


default_config = read_yaml(worflow_source_dir / "config.hg38.yaml")
configfile: get_config(default_config)
design = get_design(os.getcwd(), search_fastq_pairs)


wildcard_constraints:
    sample = r"|".join(design["Sample_id"]),
    stream = r"1|2|R1|R2"


fastq_links = link_fq(
    design.Sample_id,
    design.Upstream_file,
    design.Downstream_file
)

rule all:
    input:
        calls=expand(
            "meta_caller/calls/{sample}.vcf.gz{index}",
            sample=design["Sample_id"].tolist(),
            index=["", ".tbi"]
        ),
        html="multiqc/variant_calling_germline.html",
        tmb="TMB.tsv"
    message:
        "Finishing the WES Germline Variant Calling pipeline"


#################
### Gather QC ###
#################

rule multiqc:
    input:
        html=expand(
            "fastp/html/pe/{sample}.fastp.html",
            sample=design["Sample_id"]
        ),
        json=expand(
            "fastp/json/pe/{sample}.fastp.json",
            sample=design["Sample_id"]
        ),
        picard=expand(
            "picard/alignment_summary/{sample}.summary.txt",
            sample=design["Sample_id"]
        ),
        fastq_screen=expand(
            "fastq_screen/{sample}.{stream}.fastq_screen.{ext}",
            sample=design["Sample_id"],
            stream=["1", "2"],
            ext=["txt", "png"]
        ),
        sambamba_metrics=expand(
            "sambamba/markdup/{sample}.bam",
            sample=design["Sample_id"]
        ),
        snpeff_stats=expand(
            "snpeff/report/{sample}.html",
            sample=design["Sample_id"]
        ),
        snpeff_csvstats=expand(
            "snpeff/csvstats/{sample}.csv",
            sample=design["Sample_id"]
        )
    output:
        report(
            "multiqc/variant_calling_germline.html",
            caption="../common/reports/multiqc.rst",
            category="Quality Controls"
        )
    message:
        "Aggregating quality reports from SnpEff"
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: min(attempt * 1536, 10240),
        time_min=lambda wildcards, attempt: attempt * 35,
        tmpdir="tmp"
    log:
        "logs/multiqc.log"
    wrapper:
        "bio/multiqc"


rule alignment_summary:
    input:
        bam="sambamba/sort/{sample}.bam",
        bam_index="sambamba/sort/{sample}.bam.bai",
        ref=config['ref']['fasta'],
        ref_idx=get_fai(config['ref']['fasta']),
        ref_dict=get_dict(config['ref']['fasta']),
    output:
        temp("picard/alignment_summary/{sample}.summary.txt")
    message:
        "Collecting alignment metrics on GATK recalibrated {wildcards.sample}"
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1020,
        time_min=lambda wildcards, attempt: attempt * 45,
        tmpdir="tmp"
    log:
        "logs/picard/alignment_summary/{sample}.log"
    params:
        "VALIDATION_STRINGENCY=LENIENT "
        "METRIC_ACCUMULATION_LEVEL=null "
        "METRIC_ACCUMULATION_LEVEL=SAMPLE"
    wrapper:
        "bio/picard/collectalignmentsummarymetrics"


rule fastq_screen:
    input:
        "reads/{sample}.{stream}.fq.gz"
    output:
        txt=temp("fastq_screen/{sample}.{stream}.fastq_screen.txt"),
        png=temp("fastq_screen/{sample}.{stream}.fastq_screen.png")
    message:
        "Assessing quality of {wildcards.sample}, {wildcards.stream}"
    threads: config.get("threads", 20)
    resources:
        mem_mb=lambda wildcard, attempt: min(attempt * 4096, 8192),
        time_min=lambda wildcard, attempt: attempt * 50,
        tmpdir="tmp"
    params:
        fastq_screen_config=config["fastq_screen"],
        subset=100000,
        aligner='bowtie2'
    log:
        "logs/fastqc/{sample}.{stream}.log"
    wrapper:
        "bio/fastq_screen"


##############################
### Tumor Molecular Burden ###
##############################

tmb_config = {
    "bed": config["ref"]["capture_kit_bed"],
    "filter_in": config["tmb"].get("filter_in", []),
    "filter_out": config["tmb"].get("filter_out", []),
    "min_coverage": config["tmb"].get("min_coverage", []),
    "allele_depth_keyname": config["tmb"].get("allele_depth_keyname", "AD"),
    "tmb_highness_threshold": config["tmb"].get("tmb_highness_threshold", 10),
    "sample_list": design.Sample_id.tolist()
}

module tmb_meta:
    snakefile: "../../meta/bio/somatic_tmb/test/Snakefile"
    config: tmb_config


use rule * from tmb_meta


#################################
### FINAL VCF FILE INDEXATION ###
#################################

module compress_index_vcf_meta:
    snakefile: "../../meta/bio/compress_index_vcf/test/Snakefile"
    config: config

use rule * from compress_index_vcf_meta as compress_index_vcf_*

######################
### VCF annotation ###
######################

# rule annotate_vcf:
#     input:
#         design="design.tsv",
#         calls=expand(
#             "mutect2/corrected/{sample}.vcf.gz", sample=design["Sample_id"]
#         ),
#         calls_index=expand(
#             get_tbi("mutect2/corrected/{sample}.vcf.gz"),
#             sample=design["Sample_id"]
#         ),
#     output:
#         calls=temp(expand(
#             "snpeff_snpsift/snpsift/fixed/{sample}.vcf.gz",
#             sample=design["Sample_id"]
#         )),
#         calls_index=temp(expand(
#             "snpeff_snpsift/snpsift/fixed/{sample}.vcf.gz.tbi",
#             sample=design["Sample_id"]
#         )),
#         table=temp(expand(
#             "snpeff_snpsift/snpsift/extractFields/{sample}.tsv",
#             sample=design["Sample_id"]
#         )),
#         html="snpeff_snpsift/multiqc/SnpEff_annotation.html",
#         html_data=directory("snpeff_snpsift/multiqc/SnpEff_annotation_data")
#     message:
#         "Annotating VCF"
#     threads: 2
#     resources:
#         mem_mb=lambda wildcards, attempt: attempt * 1024 * 5,
#         time_min=lambda wildcards, attempt: attempt * 60 * 4,
#         tmpdir="tmp"
#     handover: True
#     log:
#         "logs/snpeff_snpsift_pipeline.log"
#     params:
#         mkdir="--parents --verbose",
#         ln="--symbolic --force --relative --verbose",
#         variant_dir="mutect2/corrected/",
#         outdir="snpeff_snpsift",
#         pipeline_path="/mnt/beegfs/pipelines/snakemake-wrappers/bigr_pipelines/snpeff_snpsift/run.sh"
#     shell:
#         "mkdir {params.mkdir} {params.outdir} > {log} 2>&1 && "
#         "ln {params.ln} {input.config} {params.outdir} >> {log} 2>&1 && "
#         "ln {params.ln} {params.variant_dir} {params.outdir} >> {log} 2>&1 && "
#         "cd {params.outdir} && "
#         "bash {params.pipeline_path} >> {log} 2>&1 && "
#         "mv --force --verbose snpeff_snpsift/* . >> {log} 2>&1"

rule snpeff_snpsift_pipeline:
    input:
        calls=expand(
            "meta_caller/calls/{sample}.vcf.gz{index}",
            sample=design["Sample_id"].tolist(),
            index=["", ".tbi"]
        ),
        config="config.yaml"
    output:
        snpeff_stats=expand(
            "snpeff/report/{sample}.html",
            sample=design["Sample_id"]
        ),
        snpeff_csvstats=expand(
            "snpeff/csvstats/{sample}.csv",
            sample=design["Sample_id"]
        ),
        fixed=expand(
            "snpsift/fixed/{sample}.vcf.gz{index}",
            sample=design["Sample_id"],
            index=["", ".tbi"]
        )
    handover: True
    shadow: 'shallow'
    message: "Annotating VCF files"
    params:
        organism = config["params"].get("organism", "hg38"),
        mkdir="--parents --verbose",
        ln="--symbolic --force --relative --verbose",
        variant_dir="mutect2/corrected/",
        outdir="snpeff_snpsift",
        pipeline_path=config.get(
            "snpeff_snpsift_run_path",
            "/mnt/beegfs/pipelines/snakemake-wrappers/bigr_pipelines/snpeff_snpsift/run.sh"
        )
    log:
        "log/snpeff_snpsift_pipeline.log"
    shell:
        "mkdir {params.mkdir} {params.outdir}/data_input/calls/ > {log} 2>&1 && "
        "ln {params.ln} {input.config} {params.outdir} >> {log} 2>&1 && "
        "ln {params.ln} {params.variant_dir}/* {params.outdir}/data_input/calls/ >> {log} 2>&1 && "
        "cd {params.outdir} && "
        "bash {params.pipeline_path} {params.organism} | tee -a ${{OLDPWD}}/{log} 2>&1 && "
        "ln {params.ln} snpsift/ ${{OLDPWD}}/snpsift >> {log} 2>&1 && "
        "ln {params.ln} snpeff/ ${{OLDPWD}}/snpsift >> {log} 2>&1 "



#####################################
### Merge variant calling results ###
#####################################

module metacaller_germline_meta:
    snakefile: "../../meta/bio/meta_caller_germline/test/Snakefile"
    config: {"genome": config["ref"]["fasta"], "bed": config["ref"]["capture_kit_bed"]}


use rule * from metacaller_germline_meta as *


############################################################################
### Correcting Mutect2 :                                                 ###
### AS_FilterStatus: Number=1 and not Number=A which violates VCF format ###
### AD becomes ADM: AD is reserved for Allele Depth, Mutect2 stores      ###
###                 multiple information under "AD" field.               ###
############################################################################

rule correct_mutect2_vcf:
    input:
        "mutect2/filter_reheaded/{sample}.vcf.gz"
    output:
        temp("mutect2/corrected/{sample}.vcf")
    message:
        "Renaming reserved AD field and fixing AS_FilterStrand format error"
        " on {wildcards.sample}"
    threads: 3
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 256,
        time_min=lambda wildcards, attempt: attempt * 20,
        tmpdir="tmp"
    log:
        "logs/mutect2/correct_fields/{sample}.log"
    params:
        rename_ad="'s/=AD;/=ADM;/g'",
        rename_ad_format="'s/:AD:/:ADM:/g'",
        fix_as_filterstatus="'s/ID=AS_FilterStatus,Number=A/ID=AS_FilterStatus,Number=1/g'"
    shell:
        "(gunzip -c {input} | "
        "sed {params.rename_ad} | "
        "sed {params.rename_ad_format} | "
        "sed {params.fix_as_filterstatus}) "
        "> {output} 2> {log}"

###############################
### Variant calling Mutect2 ###
###############################

gatk_mutect2_germline_meta_config = {
    "genome": config["ref"]["fasta"],
    "known": config["ref"]["af_only"],
    "bed": config["ref"]["capture_kit_bed"],
    "dbsnp": config["ref"]["dbsnp"],
    "sample_list": design["Sample_id"].to_list()
}

module gatk_mutect2_germline_meta:
    snakefile: "../../meta/bio/mutect2_germline/test/Snakefile"
    config: gatk_mutect2_germline_meta_config

use rule * from gatk_mutect2_germline_meta


use rule muterc2_filter from gatk_mutect2_germline_meta with:
    input:
        vcf="mutect2/call/{sample}.vcf.gz",
        ref=config["ref"]["fasta"],
        fasta_index=get_fai(config["ref"]["fasta"]),
        fasta_dict=get_dict(config["ref"]["fasta"]),
        contamination="summary/{sample}_calculate_contamination.table",
        bam="sambamba/markdup/{sample}.bam",
        bam_index=get_bai("sambamba/markdup/{sample}.bam"),
        f1r2="gatk/artifacts_prior/{sample}.artifacts_prior.tar.gz"


use rule get_pileup_summaries from gatk_mutect2_germline_meta with:
    input:
        bam="sambamba/markdup/{sample}.bam",
        bam_index=get_bai("sambamba/markdup/{sample}.bam"),
        intervals=config["ref"]["capture_kit_bed"],
        variants=config["ref"]["af_only"],
        variants_index=get_tbi(config["ref"]["af_only"])


use rule mutect2_germline from gatk_mutect2_germline_meta with:
    input:
        fasta=config["ref"]["fasta"],
        fasta_index=get_fai(config["ref"]["fasta"]),
        fasta_dict=get_dict(config["ref"]["fasta"]),
        map="sambamba/markdup/{sample}.bam",
        map_index=get_bai("sambamba/markdup/{sample}.bam"),
        germline=config["ref"]["af_only"],
        germline_tbi=get_tbi(config["ref"]["af_only"]),
        intervals=config["ref"]["capture_kit_bed"]

################################
### Variant Calling Varscan2 ###
################################

varscan2_config = {
    "genome": config["ref"]["fasta"],
    "bed": config["ref"]["capture_kit_bed"]
}

module varscan2_meta:
    snakefile: "../../meta/bio/varscan2_germline/test/Snakefile"
    config: varscan2_config

use rule * from varscan2_meta as varscan2_meta_*


###################
### BWA MAPPING ###
###################

module bwa_meta:
    snakefile: "../../meta/bio/bwa_fixmate/test/Snakefile"
    config: {"threads": config["threads"], "genome": config["ref"]["fasta"]}

use rule * from bwa_meta as *

use rule bwa_mem from bwa_meta with:
    input:
        reads=expand(
            "fastp/trimmed/pe/{sample}.{stream}.fastq",
            stream=["1", "2"],
            allow_missing=True
        ),
        index=multiext(
            "bwa_mem2/index/genome", ".0123", ".amb", ".ann", ".pac"
        )


#####################
### Deduplicating ###
#####################

rule sambamba_markduplicates:
    input:
        bam="sambamba/sort/{sample}.bam",
        bai=get_bai("sambamba/sort/{sample}.bam")
    output:
        bam=temp("sambamba/markdup/{sample}.bam")
    message:
        "Removing duplicates on {wildcards.sample}"
    threads: 6
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 10240,
        time_min=lambda wildcards, attempt: attempt * 45,
        tmpdir="tmp"
    log:
        "logs/sambamba/markduplicates/{sample}.log"
    params:
        extra = config["sambamba"].get(
            "markdup", "--remove-duplicates --overflow-list-size 600000"
        )
    wrapper:
        "bio/sambamba/markdup"


use rule sambamba_index from bwa_meta with:
    input:
        "{tool}/{command}/{sample}.bam"
    output:
        temp("{tool}/{command}/{sample}.bam.bai")
    message:
        "Indexing mapped reads of {wildcards.sample} ({wildcards.tool}/{wildcards.command})"
    log:
        "logs/sambamba/sort/{sample}_{tool}_{command}.log"
    params:
        extra = ""


##############################
### GATK BAM RECALIBRATION ###
##############################

gatk_bqsr_config = {
    "threads": config["threads"],
    "genome": config["ref"]["fasta"],
    "dbsnp": config["ref"]["dbsnp"]
}

module gatk_bqsr_meta:
    snakefile: "../../meta/bio/gatk_bqsr/test/Snakefile"
    config: gatk_bqsr_config


use rule gatk_apply_baserecalibrator from gatk_bqsr_meta with:
    input:
        bam="sambamba/markdup/{sample}.bam",
        bam_index=get_bai("sambamba/markdup/{sample}.bam"),
        ref=config['ref']['fasta'],
        ref_idx=get_fai(config['ref']['fasta']),
        ref_dict=get_dict(config['ref']['fasta']),
        recal_table="gatk/recal_data_table/{sample}.grp"


use rule gatk_compute_baserecalibration_table from gatk_bqsr_meta with:
    input:
        bam="sambamba/markdup/{sample}.bam",
        bam_index=get_bai("sambamba/markdup/{sample}.bam"),
        ref=config['ref']['fasta'],
        ref_idx=get_fai(config['ref']['fasta']),
        ref_dict=get_dict(config['ref']['fasta']),
        known=config['ref']['dbsnp'],
        known_idx=get_tbi(config['ref']['dbsnp'])


############################
### FASTP FASTQ CLEANING ###
############################

rule fastp_clean:
    input:
        sample=expand(
            "reads/{sample}.{stream}.fq.gz",
            stream=["1", "2"],
            allow_missing=True
        ),
    output:
        trimmed=expand(
            "fastp/trimmed/pe/{sample}.{stream}.fastq",
            stream=["1", "2"],
            allow_missing=True
        ),
        html="fastp/html/pe/{sample}.fastp.html",
        json=temp("fastp/json/pe/{sample}.fastp.json")
    message: "Cleaning {wildcards.sample} with Fastp"
    threads: 1
    resources:
        mem_mb=lambda wildcard, attempt: min(attempt * 4096, 15360),
        time_min=lambda wildcard, attempt: attempt * 45,
        tmpdir="tmp"
    params:
        adapters=config.get("fastp_adapters", None),
        extra=config.get("fastp_extra", "")
    log:
        "logs/fastp/{sample}.log"
    wrapper:
        "bio/fastp"


#################################################
### Gather files from iRODS or mounting point ###
#################################################

rule bigr_copy:
    output:
        "reads/{sample}.{stream}.fq.gz"
    message:
        "Gathering {wildcards.sample} fastq file ({wildcards.stream})"
    threads: 1
    resources:
      mem_mb=lambda wildcards, attempt: min(attempt * 1024, 2048),
      time_min=lambda wildcards, attempt: attempt * 45,
    params:
        input=lambda wildcards, output: fastq_links[output[0]]
    log:
        "logs/bigr_copy/{sample}.{stream}.log"
    wrapper:
        "bio/BiGR/copy"
