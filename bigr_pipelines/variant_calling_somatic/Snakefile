import datetime
import logging
import os
import pandas
import sys
from pathlib import Path

worflow_source_dir = Path(next(iter(workflow.get_sources()))).absolute().parent
common = str(worflow_source_dir / "../common/python")
sys.path.append(common)

from file_manager import *
from files_linker import *
from write_yaml import *
from messages import *
from snakemake.utils import min_version
min_version("6.0")

logging.basicConfig(
    filename="snakemake.variant_calling_somatic.log",
    filemode="w",
    level=logging.DEBUG
)

container: "docker://continuumio/miniconda3:4.4.10"
localrules: bigr_copy

ruleorder: sambamba_index_bam > sambamba_index
ruleorder: gatk_filter_mutect_calls > tabix_index
ruleorder: mutect2_somatic > tabix_index

default_config = read_yaml(worflow_source_dir / "config.hg38.yaml")
configfile: get_config(default_config)
design = get_design(os.getcwd(), search_fastq_somatic)
#design = design.head(2).tail(1)
design.dropna(inplace=True)
#print(design)

design.index = design["Sample_id"]
#design.drop(index="s070", inplace=True)

wildcard_constraints:
    sample = r"|".join(design["Sample_id"]),
    stream = r"1|2|R1|R2",
    status = r"normal|tumor",
    content = r"snp|indel"


fastq_links = link_fq_somatic(
    sample_names=design.Sample_id,
    n1_paths=design.Upstream_file_normal,
    t1_paths=design.Upstream_file_tumor,
    n2_paths=design.Downstream_file_normal,
    t2_paths=design.Downstream_file_tumor,
)

ruleorder: fix_annotation_for_gatk > pbgzip_compress
ruleorder: gatk_variant_filtration > pbgzip_compress

rule all:
    input:
        #maf="maf/complete.maf",
        #mafs=expand(
        #    "maf/maftools/{sample}.maf",
        #    sample=design["Sample_id"].tolist()
        #),
        #calls=expand(
        #   "maf/occurence_annotated/{sample}.vcf.gz{index}",
        #   sample=design["Sample_id"].tolist(),
        #   index=["", ".tbi"]
        # ),
        mutect2=expand(
            "bcftools/mutect2/{sample}.vcf.gz",
            sample=design["Sample_id"].tolist()
        ),
        mutect2_tbi=expand(
            "bcftools/mutect2/{sample}.vcf.gz.tbi",
            sample=design["Sample_id"].tolist()
        ),
        annotated_vcf=expand(
            "bigr/f2i/{sample}.vcf.gz",
            sample=design["Sample_id"].tolist()
        ),
        annotated_vcf_tbi=expand(
            "bigr/f2i/{sample}.vcf.gz.tbi",
            sample=design["Sample_id"].tolist()
        ),
        facets=expand(
            "facets/{sample}/{sample}.{ext}",
            sample=design["Sample_id"].tolist(),
            ext=["vcf.gz", "cnv.png", "cov.pdf", "spider.pdf", "csv.gz"]
        ),
        #varscan2=expand(
        #    "bcftools/varscan2/{sample}.vcf.gz",
        #    sample=design["Sample_id"].tolist()
        #),
        #varscan2_tbi=expand(
        #    "bcftools/varscan2/{sample}.vcf.gz.tbi",
        #    sample=design["Sample_id"].tolist()
        #),
        msisensor=expand(
            "msisensor/{sample}/{sample}.msi",
            sample=design["Sample_id"].tolist()
        ),
        qc="multiqc/variant_calling_somatic.html",
        #calling_result="final.vcf.list"
    message:
        "Finishing the WES Somatic Variant Calling"



##################
### VCF to MAF ###
##################

vcf_post_process_config = {
    "ncbi_build": config["params"].get("ncbi_build", "GRCh38"),
    "center": config["params"].get("center", "GustaveRoussy"),
    "annotation_tag": "ANN=",
    "sample_list": design["Sample_id"].to_list(),
    "genome": config["ref"]["fasta"],
    "known": config["ref"]["dbsnp"],
    "chr": config["params"]["chr"]
}

module vcf_post_process:
    snakefile: "../../meta/bio/vcf_post_process/test/Snakefile"
    config: vcf_post_process_config

use rule * from vcf_post_process


#################
### Gather QC ###
#################

rule multiqc:
    input:
        html=expand(
            "fastp/html/pe/{sample}_{status}.fastp.html",
            sample=design["Sample_id"],
            status=["normal", "tumor"]
        ),
        json=expand(
            "fastp/json/pe/{sample}_{status}.fastp.json",
            sample=design["Sample_id"],
            status=["normal", "tumor"]
        ),
        sambamba_metrics=expand(
            "sambamba/markdup/{sample}_{status}.bam",
            sample=design["Sample_id"],
            status=["normal", "tumor"]
        ),
        fastq_screen=expand(
            "fastq_screen/{sample}.{stream}.{status}.fastq_screen.{ext}",
            sample=design["Sample_id"],
            stream=["1", "2"],
            ext=["txt", "png"],
            status=["normal", "tumor"]
        ),
        picard_summary=expand(
            "picard/alignment_summary/{sample}_{status}.summary.txt",
            sample=design["Sample_id"],
            status=["normal", "tumor"]
        )
    output:
        report(
            "multiqc/variant_calling_somatic.html",
            caption="../common/reports/multiqc.rst",
            category="Quality Controls"
        )
    message:
        "Aggregating quality reports from SnpEff"
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: min(attempt * 1536, 10240),
        time_min=lambda wildcards, attempt: attempt * 35,
        tmpdir="tmp"
    log:
        "logs/multiqc.log"
    wrapper:
        "bio/multiqc"


rule alignment_summary:
    input:
        bam="sambamba/sort/{sample}_{status}.bam",
        bam_index=get_bai("sambamba/sort/{sample}_{status}.bam"),
        ref=config['ref']['fasta'],
        ref_idx=get_fai(config['ref']['fasta']),
        ref_dict=get_dict(config['ref']['fasta']),
    output:
        temp("picard/alignment_summary/{sample}_{status}.summary.txt")
    message:
        "Collecting alignment metrics on GATK recalibrated {wildcards.sample}"
        " (considering {wildcards.status})"
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1020,
        time_min=lambda wildcards, attempt: attempt * 45,
        tmpdir="tmp"
    log:
        "logs/picard/alignment_summary/{sample}_{status}.log"
    params:
        "VALIDATION_STRINGENCY=LENIENT "
        "METRIC_ACCUMULATION_LEVEL=null "
        "METRIC_ACCUMULATION_LEVEL=SAMPLE"
    wrapper:
        "bio/picard/collectalignmentsummarymetrics"


rule fastq_screen:
    input:
        "reads/{status}/{sample}.{stream}.fq.gz"
    output:
        txt=temp("fastq_screen/{sample}.{stream}.{status}.fastq_screen.txt"),
        png=temp("fastq_screen/{sample}.{stream}.{status}.fastq_screen.png")
    message:
        "Assessing quality of {wildcards.sample}, {wildcards.stream}"
        " (considering {wildcards.status})"
    threads: config.get("threads", 20)
    resources:
        mem_mb=lambda wildcard, attempt: min(attempt * 1024 * 8, 20480),
        time_min=lambda wildcard, attempt: attempt * 75,
        tmpdir="tmp"
    params:
        fastq_screen_config=config["fastq_screen"],
        subset=100000,
        aligner='bowtie2'
    log:
        "logs/fastqc/{sample}.{stream}.{status}.log"
    wrapper:
        "bio/fastq_screen"


######################
### MSI sensor pro ###
######################

msi_sensor_config = {
    "fasta": config["ref"]["fasta"],
    "bed": config["ref"]["capture_kit_bed"],
    "msi_scan_extra": config["msisensor_pro"].get("scan", ""),
    "msi_pro_extra": config["msisensor_pro"].get("msi", "")
}

module missensor_pro_meta:
    snakefile: "../../meta/bio/msi_sensor_pro/test/Snakefile"
    config: msi_sensor_config

use rule * from missensor_pro_meta

##################
### CNV Facets ###
##################


rule add_chr_to_pileup:
    input:
        "samtools/mpileup/{sample}.mpileup.gz"
    output:
        temp("samtools/mpileup/{sample}.chr.mpileup.gz")
    message:
        "Adding 'chr' to sequences on {wildcards.sample}"
    threads: 3
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 512,
        time_min=lambda wildcards, attempt: attempt * 75,
        tmpdir="tmp"
    log:
        "logs/cnv_facets/add_chr/{sample}.log"
    params:
        gzip = "-c",
        awk = 'BEGIN {FS="\\t"; print "Chromosome\\tPosition\\tRef\\tAlt\\tFile1R\\tFile1A\\tFile1E\\tFile1D\\tFile2R\\tFile2A\\tFile2E\\tFile2D"} {if ($1 !~ /^chr*/) {print "chr"$0} else {print $0}}'
    shell:
        "gunzip {params.gzip} {input} | "
        "awk '{params.awk}' | "
        "gzip {params.gzip} > {output} 2> {log}"


rule cnv_facets:
    input:
        #pileup="samtools/mpileup/{sample}.chr.mpileup.gz",
        tumor_bam="sambamba/markdup/{sample}_tumor.bam",
        normal_bam="sambamba/markdup/{sample}_normal.bam",
        vcf=config["ref"]["dbsnp"],
        vcf_index=get_tbi(config["ref"]["dbsnp"]),
        bed=config["ref"]["capture_kit_bed"]
    output:
        vcf="facets/{sample}/{sample}.vcf.gz",
        profile="facets/{sample}/{sample}.cnv.png",
        coverage="facets/{sample}/{sample}.cov.pdf",
        spider="facets/{sample}/{sample}.spider.pdf",
        pileup="facets/{sample}/{sample}.csv.gz"
    message:
        "Searching for CNV in {wildcards.sample} with Facets"
    threads: 10
    resources:
        mem_mb=lambda wildcards, attempt: (attempt * 1024 * 20) + 102400,
        time_min=lambda wildcards, attempt: attempt * 60 * 2,
        tmpdir="tmp"
    params:
        extra=config.get(
            "facets_extra",
            "--snp-count-orphans --gbuild hg38 --nbhd-snp 250"
        ),
        prefix="facets/{sample}/{sample}"
    log:
        "logs/facets/cnv/{sample}.log"
    wrapper:
        "bio/facets/cnv"


#################################
### FINAL VCF FILE INDEXATION ###
#################################

module compress_index_vcf_meta:
    snakefile: "../../meta/bio/compress_index_vcf/test/Snakefile"
    config: config

use rule * from compress_index_vcf_meta


use rule tabix_index from compress_index_vcf_meta as snp_indel_tabix_index with:
    input:
        "{tool}/{subcommand}/{sample}.{content}.vcf.gz"
    output:
        "{tool}/{subcommand}/{sample}.{content}.vcf.gz.tbi"
    message:
        "Indexing {wildcards.sample} (from somatic varscan "
        "{wildcards.content}) with tabix."
    log:
        "logs/{tool}/{subcommand}/tabix/index/{sample}.{content}.log"


use rule pbgzip_compress from compress_index_vcf_meta as si_pbgzip with:
    input:
        "{tool}/{subcommand}/{sample}.{content}.vcf"
    output:
        "{tool}/{subcommand}/{sample}.{content}.vcf.gz"
    message:
        "Compressnig {wildcards.sample} (from somatic varscan "
        "{wildcards.content}) with pbgzip."
    log:
        "logs/{tool}/{subcommand}/pgbzip/varcsanc2/{sample}.{content}.log"


######################
### VCF annotation ###
######################

rule vcf_to_tsv_somatic:
    input:
        call="bigr/f2i/{sample}.vcf.gz",
        call_index="bigr/f2i/{sample}.vcf.gz.tbi"
    output:
        tsv="snpsift/extractAllFields/{sample}.tsv"
    threads: 2
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 10240,
        time_min=lambda wildcards, attempt: attempt * 25,
        tmpdir="tmp"
    log:
        "logs/snpsift/extract_all_fields/{sample}.log"
    params:
        extra=config["snpeff_snpsift"].get(
            "vcf_to_tsv_extra", "-e '.' -s ';'"
        )
    wrapper:
        "bio/snpsift/extractAllFields"


rule format_to_info_somatic:
    input:
        call = "snpeff_snpsift/snpsift/fixed/{sample}.vcf"
    output:
        call = temp("bigr/f2i/{sample}.vcf")
    message:
        "Moving format fields to info for {wildcards.sample}"
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 2048,
        time_min=lambda wildcards, attempt: attempt * 45,
        tmpdir="tmp",
        #partition="visuq",
        #grep="gpu:T4:1"
    params:
        normal_sample=lambda wildcards: f"{wildcards.sample}_normal",
        tumor_sample=lambda wildcards: f"{wildcards.sample}_tumor"
    log:
        "logs/vcf_format_to_info/{sample}.log"
    wrapper:
        "bio/BiGR/vcf_format_to_info"


rule gunzip_annotated_vcf:
    input:
        "snpeff_snpsift/snpsift/fixed/{sample}.vcf.gz"
    output:
        temp("snpeff_snpsift/snpsift/fixed/{sample}.vcf")
    message:
        "Temporary unzipping for manual edition of {wildcards.sample}'s VCF"
    threads: 1
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 512,
        time_min=lambda wildcards, attempt: attempt * 45,
        tmpdir="tmp"
    log:
        "logs/gunzip/{sample}_annotated.log"
    params:
        config.get("gunzip_extra", "--to-stdout --decompress --force --verbose")
    shell:
        "gunzip {params} {input} > {output} 2> {log}"



rule annotate_vcf:
    input:
        design="design.tsv",
        config="config.yaml",
        calls=expand(
            "mutect2/corrected/{sample}.vcf.gz",
            sample=design["Sample_id"]
        ),
        calls_index=expand(
            get_tbi("mutect2/corrected/{sample}.vcf.gz"),
            sample=design["Sample_id"]
        ),
    output:
        calls=temp(expand(
            "snpeff_snpsift/snpsift/fixed/{sample}.vcf.gz",
            sample=design["Sample_id"]
        )),
        calls_index=temp(expand(
            "snpeff_snpsift/snpsift/fixed/{sample}.vcf.gz.tbi",
            sample=design["Sample_id"]
        )),
        table=temp(expand(
            "snpeff_snpsift/snpsift/extractFields/{sample}.tsv",
            sample=design["Sample_id"]
        )),
        html="snpeff_snpsift/multiqc/SnpEff_annotation.html",
        html_data=directory("snpeff_snpsift/multiqc/SnpEff_annotation_data")
    message:
        "Annotating VCF"
    threads: 2
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1024 * 5,
        time_min=lambda wildcards, attempt: attempt * 60 * 4,
        tmpdir="tmp"
    handover: True
    log:
        "logs/snpeff_snpsift_pipeline.log"
    params:
        mkdir="--parents --verbose",
        ln="--symbolic --force --relative --verbose",
        variant_dir="mutect2/corrected/",
        outdir="snpeff_snpsift",
        pipeline_path=config.get(
            "snpeff_snpsift_run_path",
            "/mnt/beegfs/pipelines/snakemake-wrappers/bigr_pipelines/snpeff_snpsift/run.sh"
        ),
        organism = config["params"].get("organism", "hg38")
    shell:
        "mkdir {params.mkdir} {params.outdir}/data_input/calls/ > {log} 2>&1 && "
        "ln {params.ln} {input.config} {params.outdir} >> {log} 2>&1 && "
        "ln {params.ln} {params.variant_dir}/* {params.outdir}/data_input/calls/ >> {log} 2>&1 && "
        "cd {params.outdir} && "
        "bash {params.pipeline_path} {params.organism} | tee -a ${{OLDPWD}}/{log} 2>&1"


#####################################
### Merge variant calling results ###
#####################################

# module metacaller_somatic_meta:
#     snakefile: "../../meta/bio/meta_caller_somatic/test/Snakefile"
#     config: {"genome": config["ref"]["fasta"], "bed": config["ref"]["capture_kit_bed"]}
#
#
# use rule * from metacaller_somatic_meta as *


############################################################################
### Correcting Mutect2 :                                                 ###
### AS_FilterStatus: Number=1 and not Number=A which violates VCF format ###
############################################################################

rule correct_mutect2_vcf:
    input:
        "bcftools/mutect2/{sample}.vcf.gz"
    output:
        temp("mutect2/corrected/{sample}.vcf")
    message:
        "Fixing AS_FilterStrand format error"
        " on {wildcards.sample}"
    threads: 2
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 256,
        time_min=lambda wildcards, attempt: attempt * 20,
        tmpdir="tmp"
    log:
        "logs/mutect2/correct_fields/{sample}.log"
    params:
        fix_as_filterstatus="'s/ID=AS_FilterStatus,Number=A/ID=AS_FilterStatus,Number=1/g'"
    shell:
        "(gunzip -c {input} | "
        "sed {params.fix_as_filterstatus}) "
        "> {output} 2> {log}"

###############################
### Variant calling Mutect2 ###
###############################

gatk_mutect2_somatic_config = {
    "genome": config["ref"]["fasta"],
    "known": config["ref"]["af_only"],
    "bed": config["ref"]["capture_kit_bed"],
    "dbsnp": config["ref"]["dbsnp"],
    "sample_list": design["Sample_id"].to_list(),
    "chrom": config["params"]["chr"]
}


module gatk_mutect2_somatic_meta:
    snakefile: "../../meta/bio/mutect2_somatic/test/Snakefile"
    config: gatk_mutect2_somatic_config


use rule * from gatk_mutect2_somatic_meta


################################
### Variant Calling Varscan2 ###
################################

varscan2_somatic_config = {
    "genome": config["ref"]["fasta"],
    "bed": config["ref"]["capture_kit_bed"]
}

module varscan2_somatic_meta:
    snakefile: "../../meta/bio/varscan2_somatic/test/Snakefile"
    config: varscan2_somatic_config

use rule * from varscan2_somatic_meta


##############################
### GATK BAM RECALIBRATION ###
##############################

gatk_bqsr_config = {
    "threads": config["threads"],
    "genome": config["ref"]["fasta"],
    "dbsnp": config["ref"]["dbsnp"],
    "base_recal_extra": "",
    "apply_base_recal_extra": config.get(
        "gatk", {"apply_base_recal_extra": "--create-output-bam-index"}
    ).get("apply_base_recal_extra", "--create-output-bam-index")
}

module gatk_bqsr_meta:
    snakefile: "../../meta/bio/gatk_bqsr/test/Snakefile"
    config: gatk_bqsr_config


use rule gatk_apply_baserecalibrator from gatk_bqsr_meta with:
    input:
        bam="sambamba/markdup/{sample}_{status}.bam",
        bam_index=get_bai("sambamba/markdup/{sample}_{status}.bam"),
        ref=config['ref']['fasta'],
        ref_idx=get_fai(config['ref']['fasta']),
        ref_dict=get_dict(config['ref']['fasta']),
        recal_table="gatk/recal_data_table/{sample}_{status}.grp"
    output:
        bam="gatk/recal_bam/{sample}_{status}.bam",
        bai=get_bai("gatk/recal_bam/{sample}_{status}.bam")
    message:
        "Applying BQSR on {wildcards.status} {wildcards.sample} with GATK"
    params:
        extra=config.get(
            "gatk", {"apply_base_recal_extra", "--create-output-bam-index"}
        ).get("apply_base_recal_extra", "--create-output-bam-index")
    log:
        "logs/gatk/applybqsr/{sample}.{status}.log"


use rule gatk_compute_baserecalibration_table from gatk_bqsr_meta with:
    input:
        bam="sambamba/markdup/{sample}_{status}.bam",
        bam_index=get_bai("sambamba/markdup/{sample}_{status}.bam"),
        ref=config['ref']['fasta'],
        ref_idx=get_fai(config['ref']['fasta']),
        ref_dict=get_dict(config['ref']['fasta']),
        known=config['ref']['dbsnp'],
        known_idx=get_tbi(config['ref']['dbsnp'])
    output:
        recal_table=temp("gatk/recal_data_table/{sample}_{status}.grp")
    message:
        "Compute BQSR table from {wildcards.status} {wildcards.sample} "
        "with GATK"
    log:
        "logs/gatk3/compute_bqsr/{sample}.{status}.log"


#####################
### Deduplicating ###
#####################

rule sambamba_markduplicates:
    input:
        bam="samtools/filter/{sample}_{status}.bam",
        bai=get_bai("samtools/filter/{sample}_{status}.bam")
    output:
        bam=temp("sambamba/markdup/{sample}_{status}.bam")
    message:
        "Removing duplicates on {wildcards.sample} ({wildcards.status})"
    threads: 10
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 10240,
        time_min=lambda wildcards, attempt: attempt * 45,
        tmpdir="tmp"
    log:
        "logs/sambamba/markduplicates/{sample}_{status}.log"
    params:
        extra = config.get(
            "sambamba", {"markdup": "--remove-duplicates"}
        ).get("markdup", "--remove-duplicates")
    wrapper:
        "bio/sambamba/markdup"



"""
Filter a bam over the capturekit bed file
"""
rule samtools_filter_bed:
    input:
        "sambamba/sort/{sample}_{status}.bam"
        fasta=config["genome"],
        fasta_idx=get_fai(config["genome"]),
        fasta_dict=get_dict(config["genome"]),
        bed=config["bed"]
    output:
        temp("samtools/filter/{sample}_{status}.bam")
    threads: 10
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 2048,
        time_min=lambda wildcards, attempt: attempt * 15,
        tmpdir="tmp"
    params:
        extra="--bam --with-header"
    log:
        "logs/samtools/filter/{sample}_{status}.log"
    wrapper:
        "bio/samtools/view"


###################
### BWA MAPPING ###
###################

module bwa_fixmate_meta:
    snakefile: "../../meta/bio/bwa_fixmate/test/Snakefile"
    config: {"threads": config["threads"], "genome": config["ref"]["fasta"]}


def get_best_bwa_index():
    """Return cached data if available"""
    if config["ref"]["fasta"] == "/mnt/beegfs/database/bioinfo/Index_DB/Fasta/Ensembl/GRCh38.99/GRCh38.99.homo_sapiens.dna.main_chr.fasta":
        return multiext(
            "bwa_mem2/index/genome.hg38", ".0123", ".amb", ".ann", ".pac"
        )
    elif config["ref"]["fasta"] == "/mnt/beegfs/database/bioinfo/Index_DB/Fasta/Ensembl/GRCh37.75/Homo_sapiens.GRCh37.75.dna.primary_assembly.fa":
        return multiext(
            "bwa_mem2/index/genome.hg19", ".0123", ".amb", ".ann", ".pac"
        )
    elif config["ref"]["fasta"] == "/mnt/beegfs/database/bioinfo/Index_DB/Fasta/Ensembl/GRCm38.99/GRCm38.99.mus_musculus.dna.fasta":
        return multiext(
            "bwa_mem2/index/genome.mm10", ".0123", ".amb", ".ann", ".pac"
        )
    return multiext(
        "bwa_mem2/index/genome", ".0123", ".amb", ".ann", ".pac"
    )


use rule sambamba_index from bwa_fixmate_meta with:
    input:
        "sambamba/sort/{sample}_{status}.bam"
    output:
        temp("sambamba/sort/{sample}_{status}.bam.bai")
    message:
        "Indexing mapped reads of {wildcards.status} {wildcards.sample}"
    log:
        "logs/sambamba/sort/{sample}.{status}.log"


use rule sambamba_sort_coordinate from bwa_fixmate_meta with:
    input:
        mapping="samtools/fixmate/{sample}_{status}.bam"
    output:
        mapping=temp("sambamba/sort/{sample}_{status}.bam")
    message:
        "Sorting {wildcards.status} {wildcards.sample} reads by position"
    log:
        "logs/sambamba/sort/{sample}.{status}.log"


use rule samtools_fixmate from bwa_fixmate_meta with:
    input:
        "bwa_mem2/mem/{sample}_{status}.bam"
    output:
        temp("samtools/fixmate/{sample}_{status}.bam")
    message:
        "Fixing mate annotation on {wildcards.status} "
        "{wildcards.sample} with Samtools"
    log:
        "logs/samtools/fixmate/{sample}.{status}.log"


use rule bwa_mem from bwa_fixmate_meta with:
    input:
        reads=expand(
            "fastp/trimmed/pe/{sample}_{status}.{stream}.fastq",
            stream=["1", "2"],
            allow_missing=True
        ),
        index=get_best_bwa_index()
    output:
        temp("bwa_mem2/mem/{sample}_{status}.bam")
    message:
        "Mapping {wildcards.status} {wildcards.sample} with BWA"
    params:
        index=lambda wildcards, input: os.path.splitext(input["index"][0])[0],
        extra="-R '@RG\tID:{sample}_{status}\tSM:{sample}_{status}\tPU:{sample}_{status}\tPL:ILLUMINA\tCN:IGR\tDS:WES\tPG:BWA-MEM2' -M -A 2 -E 1",
        sort="samtools",         # We chose Samtools to sort by queryname
        sort_order="queryname",  # Queryname sort is needed for a fixmate
        sort_extra="-m 1536M"     # We extand the sort buffer memory
    log:
        "logs/bwa_mem2/mem/{sample}.{status}.log"

# rule bwa_samblaster_sambamba:
#     input:
#         reads=expand(
#             "fastp/trimmed/pe/{sample}_{status}.{stream}.fastq",
#             stream=["1", "2"],
#             allow_missing=True
#         ),
#         index=get_best_bwa_index()
#     output:
#         bam=temp("bwa_mem2/mem/{sample}_{status}.bam")
#     message:
#         "Mapping {wildcards.status} {wildcards.sample} with BWA, removing "
#         "duplicates with Samblaster, sorting and filtering with Sambamba."
#     threads:
#         min(config.get("threads", 20), 20)
#     threads: 1
#     resources:
#         mem_mb=lambda wildcards, attempt: attempt * 6144 + 61440,
#         time_min=lambda wildcards, attempt: attempt * 120,
#         tmpdir="tmp"
#     shadow: "shallow"
#     params:
#         index=get_best_bwa_index()[0].split('.')[0],
#         extra=lambda wildcards: f"-R '@RG\tID:{wildcards.sample}_{wildcards.status}\tSM:{wildcards.sample}_{wildcards.status}\tPU:{wildcards.sample}_{wildcards.status}\tPL:ILLUMINA\tCN:IGR\tDS:WES\tPG:BWA-MEM2' -M -A 2 -E 1",
#         sort_extra=config.get("sambamba", {"sort_extra": ""}).get("sort_extra", ""),
#         samblaster_extra=config.get("samblaster", {"extra": ""}).get("extra", ""),
#         sambamba_view_extra=config.get("sambamba", {"view_extra": "-h"}).get("view_extra", "-h")
#     log:
#         "logs/bwa_samblaster_sambamba/{sample}_{status}.log"
#




use rule bwa_index from bwa_fixmate_meta with:
    input:
        config["ref"]["fasta"]


use rule bwa_index_hg38 from bwa_fixmate_meta

use rule bwa_index_hg19 from bwa_fixmate_meta

use rule bwa_index_mm10 from bwa_fixmate_meta


############################
### FASTP FASTQ CLEANING ###
############################

rule fastp_clean:
    input:
        sample=expand(
            "reads/{status}/{sample}.{stream}.fq.gz",
            stream=["1", "2"],
            allow_missing=True
        ),
    output:
        trimmed=temp(expand(
            "fastp/trimmed/pe/{sample}_{status}.{stream}.fastq",
            stream=["1", "2"],
            allow_missing=True
        )),
        html="fastp/html/pe/{sample}_{status}.fastp.html",
        json="fastp/json/pe/{sample}_{status}.fastp.json"
    message: "Cleaning {wildcards.status} {wildcards.sample} with Fastp"
    threads: 10
    resources:
        mem_mb=lambda wildcard, attempt: min(attempt * 4096, 15360),
        time_min=lambda wildcard, attempt: attempt * 45,
        tmpdir="tmp"
    params:
        adapters=config.get("fastp_adapters", None),
        extra=config.get("fastp_extra", "")
    log:
        "logs/fastp/{sample}.{status}.log"
    wrapper:
        "bio/fastp"


#################################################
### Gather files from iRODS or mounting point ###
#################################################

rule bigr_copy:
    output:
        "reads/{status}/{sample}.{stream}.fq.gz"
    message:
        "Gathering {wildcards.status} {wildcards.sample} fastq files "
        "({wildcards.stream})"
    threads: 1
    resources:
        mem_mb=lambda wildcard, attempt: min(attempt * 1024, 2048),
        time_min=lambda wildcard, attempt: attempt * 45,
        tmpdir="tmp"
    params:
        input=lambda w, output: fastq_links[w.status][output[0]]
    log:
        "logs/bigr_copy/{status}/{sample}.{stream}.log"
    wrapper:
        "bio/BiGR/copy"


###########################
### Datasets indexation ###
###########################

index_datasets_config = {
    "genome": config["ref"]["fasta"]
}

module index_datasets:
    snakefile: "../../meta/bio/index_datasets/test/Snakefile"
    config: index_datasets_config

use rule samtools_faidx from index_datasets

use rule picard_create_sequence_dictionnary from index_datasets

rule sambamba_index_bam:
    input:
        "{tool}/{subcommand}/{sample}_{status}.bam"
    output:
        "{tool}/{subcommand}/{sample}_{status}.bam.bai"
    message:
        "Indexing {wildcards.sample} ({wildcards.status}) "
        "from {wildcards.tool}:{wildcards.subcommand}"
    threads: 8
    resources:
        mem_mb=lambda wildcards, attempt: attempt * 1024 * 16,
        time_min=lambda wildcards, attempt: attempt * 35,
        tmpdir="tmp"
    log:
        "sambamba/index/{tool}_{subcommand}/{sample}_{status}.log"
    params:
        extra = ""
    wrapper:
        "bio/sambamba/index"
